{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssdscratch/byuan48/software/anaconda3/envs/PF/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-22 14:00:45,621\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from sal.config import Config\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from vllm import SamplingParams, LLM\n",
    "from sal.utils.resampling import Particle\n",
    "\n",
    "PROMPT_HEADER = \"\"\"You are an expert mathematician assisting in solving a math problem step-by-step. You are given the original problem and the sequence of steps taken so far, which represents a *partial* solution.\n",
    "\n",
    "**Your Primary Task:**\n",
    "1.  **Review:** Carefully examine the 'Original Problem' and the 'Current Partial Solution Steps' provided below.\n",
    "2.  **Identify Issues:** Look for errors, inefficient reasoning, or lack of clarity *within the steps already taken*.\n",
    "3.  **Refine:** Provide an improved, corrected version of *only* the steps presented in 'Current Partial Solution Steps'. Make these existing steps mathematically sound and clear. Do *not* add new steps just to complete the problem unless the refinement naturally leads to the final answer based *only* on the provided partial steps.\n",
    "\n",
    "**Required Output Formatting:**\n",
    "After refining the partial steps, format your *entire output* (the refined partial solution) according to these rules:\n",
    "* **Clarity:** Present the refined steps clearly.\n",
    "* **Step-by-Step Format (for multiple steps):** If the *refined partial solution* involves multiple distinct conceptual steps (even if it's not the whole problem), use this format:\n",
    "    ```\n",
    "    ## Step 1: [Concise description of step]\n",
    "    [Brief explanation and calculations for this step]\n",
    "\n",
    "    ## Step 2: [Concise description of step]\n",
    "    [Brief explanation and calculations for this step]\n",
    "\n",
    "    ...\n",
    "    ```\n",
    "* **Concise Format (for few/simple steps):** If the *refined partial solution* is very short or consists of fewer than 3 conceptual steps (e.g., corrects a single calculation or clarifies one brief point), provide a concise explanation without the `## Step X:` structure.\n",
    "* **Final Answer Format (Conditional):** This format should ONLY be used if your refinement completes the entire problem. If the refined steps still represent a partial solution, DO NOT include this section. If the problem is fully solved by your refinement, conclude with:\n",
    "    ```\n",
    "    Therefore, the final answer is: $\\boxed{YourFinalAnswer}$. I hope it is correct.\n",
    "    ```\n",
    "    Replace `YourFinalAnswer` with the actual final result using LaTeX format for the box. **To reiterate: Only include this specific concluding line (\"Therefore, the final answer is...\") if the problem is fully solved by your refined steps. Otherwise, omit it entirely from your response.**\n",
    "\n",
    "**Focus:** Improve the mathematical accuracy and clarity of the *existing partial work* and strictly follow the specified formatting rules for the *output you generate*.\n",
    "**HINT:** Among these existing steps, Step 9 is very likely to be the first incorrect step.\n",
    "---\n",
    "**Original Problem:**\n",
    "\"\"\" # Ends right before the question is inserted\n",
    "\n",
    "PROMPT_MIDDLE = \"\"\"\n",
    "---\n",
    "**Current Partial Solution Steps:**\n",
    "\"\"\" # Ends right before the partial solution is inserted\n",
    "\n",
    "PROMPT_FOOTER = \"\"\"\n",
    "---\n",
    "\n",
    "**Your Task:** Provide the refined and correctly formatted version of the partial solution steps below. Start with the reasoning why Step 9 is wrong, and then followed with the refined steps.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIQUE_SINGLE_STEP_PROMPT = \"\"\"You are an expert mathematician reviewing a specific step in a partially solved math problem.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "1.  **Context:** Review the 'Original Problem', the 'Previous Steps' (assume these are correct), and the specific 'Step {step_number_to_analyze} Under Review'.\n",
    "2.  **Analyze:** Determine *why* 'Step {step_number_to_analyze} Under Review' is incorrect based on the preceding steps and the original problem. If it happens to be correct, state that.\n",
    "3.  **Output:** Your entire output should be ONLY:\n",
    "    * A concise explanation of the error found in Step {step_number_to_analyze}.\n",
    "    * The correctly modified text for Step {step_number_to_analyze}. Do NOT include any other steps (previous or subsequent).\n",
    "\n",
    "**Example Output Structure:**\n",
    "Reasoning: [Explain the error in Step {step_number_to_analyze} here.]\n",
    "\n",
    "Corrected Step {step_number_to_analyze}: [Provide the full text of the corrected Step {step_number_to_analyze} here.]\n",
    "\n",
    "---\n",
    "**Original Problem:**\n",
    "{question}\n",
    "---\n",
    "**Previous Steps (Steps 1 to {step_number_to_analyze_minus_1}):**\n",
    "{steps_before_suspicious_text}\n",
    "---\n",
    "**Step {step_number_to_analyze} Under Review:**\n",
    "{suspicious_step_text}\n",
    "---\n",
    "\n",
    "**Your Task:** Provide the reasoning and the single corrected step text below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_refinement_simple(\n",
    "    llm,\n",
    "    particles: List[Particle],\n",
    "    question: str,\n",
    ") -> List[Particle]:\n",
    "    \"\"\"\n",
    "    Refines particles by asking the LLM to improve its own solution. (Simplified)\n",
    "\n",
    "    Args:\n",
    "        llm: The language model instance (needs a 'generate' method).\n",
    "        particles: A list of Particle objects with a 'trajectory' attribute.\n",
    "        question: The original math problem statement.\n",
    "        temperature: Temperature for LLM generation (default: 0.7).\n",
    "\n",
    "    Returns:\n",
    "        A list of new Particle objects containing the refined solutions.\n",
    "    \"\"\"\n",
    "    refined_particles: List[Particle] = []\n",
    "\n",
    "    # Define sampling parameters for the LLM call\n",
    "    # Adjust max_tokens, top_p, stop as needed\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        max_tokens=2048, \n",
    "        stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    for particle in particles:\n",
    "        # Basic check: Skip if the particle has no prior solution steps\n",
    "        if not particle.trajectory:\n",
    "            refined_particles.append(particle.deepcopy()) # Keep the empty particle as is\n",
    "            continue\n",
    "\n",
    "        steps_before_suspicious = particle.trajectory[:8]\n",
    "        step_number_to_analyze = 9\n",
    "        step_number_to_analyze_minus_1 = 8\n",
    "        suspicious_step_text  = particle.trajectory[8]\n",
    "        steps_before_text = \"\\n\\n\".join(steps_before_suspicious) if steps_before_suspicious else \"(No previous steps)\"\n",
    "        prompt_content = CRITIQUE_SINGLE_STEP_PROMPT.format(\n",
    "            question=question,\n",
    "            step_number_to_analyze_minus_1 = step_number_to_analyze_minus_1,\n",
    "            step_number_to_analyze=step_number_to_analyze,\n",
    "            steps_before_suspicious_text=steps_before_text,\n",
    "            suspicious_step_text=suspicious_step_text\n",
    "        )\n",
    "        # ------------------------------------------\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": prompt_content}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            print(prompt)\n",
    "            outputs = llm.generate(prompt, sampling_params)\n",
    "\n",
    "            # --- Extract the refined text ---\n",
    "            # !! Adjust based on your llm.generate return structure !!\n",
    "            refined_formatted_text = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "            if not refined_formatted_text:\n",
    "                print(\"Warning: Received empty formatted refinement. Keeping original.\")\n",
    "                refined_particles.append(particle.deepcopy())\n",
    "                continue\n",
    "\n",
    "            refined_particle = particle.deepcopy()\n",
    "            # Store the refined & formatted partial solution as the new trajectory\n",
    "            # It replaces the previous (unrefined) partial trajectory\n",
    "            refined_particle.trajectory = refined_formatted_text.split(\"\\n\\n\")\n",
    "\n",
    "            refined_particles.append(refined_particle)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Basic error handling\n",
    "            print(f\"Error during formatted partial refinement: {e}. Keeping original.\")\n",
    "            refined_particles.append(particle.deepcopy())\n",
    "\n",
    "    return refined_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 14:00:51 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-22 14:00:51 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-22 14:00:51 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-22 14:00:51 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-22 14:00:53 selector.py:120] Using Flash Attention backend.\n",
      "INFO 04-22 14:00:54 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 04-22 14:00:54 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 14:00:56 model_runner.py:1099] Loading model weights took 6.0160 GB\n",
      "INFO 04-22 14:00:56 worker.py:241] Memory profiling takes 0.55 seconds\n",
      "INFO 04-22 14:00:56 worker.py:241] the current vLLM instance can use total_gpu_memory (47.43GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 04-22 14:00:56 worker.py:241] model weights take 6.02GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 35.39GiB.\n",
      "INFO 04-22 14:00:56 gpu_executor.py:76] # GPU blocks: 20705, # CPU blocks: 2340\n",
      "INFO 04-22 14:00:56 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 2.53x\n",
      "INFO 04-22 14:01:01 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 14:01:14 model_runner.py:1535] Graph capturing finished in 13 secs, took 0.19 GiB\n",
      "INFO 04-22 14:01:14 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 18.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-3.2-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Apr 2025\n",
      "\n",
      "You are an expert mathematician reviewing a specific step in a partially solved math problem.\n",
      "\n",
      "**Your Task:**\n",
      "\n",
      "1.  **Context:** Review the 'Original Problem', the 'Previous Steps' (assume these are correct), and the specific 'Step 9 Under Review'.\n",
      "2.  **Analyze:** Determine *why* 'Step 9 Under Review' is incorrect based on the preceding steps and the original problem. If it happens to be correct, state that.\n",
      "3.  **Output:** Your entire output should be ONLY:\n",
      "    * A concise explanation of the error found in Step 9.\n",
      "    * The correctly modified text for Step 9. Do NOT include any other steps (previous or subsequent).\n",
      "\n",
      "**Example Output Structure:**\n",
      "Reasoning: [Explain the error in Step 9 here.]\n",
      "\n",
      "Corrected Step 9: [Provide the full text of the corrected Step 9 here.]\n",
      "\n",
      "---\n",
      "**Original Problem:**\n",
      "What is the least positive integer multiple of 30 that can be written with only the digits 0 and 2?\n",
      "---\n",
      "**Previous Steps (Steps 1 to 8):**\n",
      "Understand the problem\n",
      "We are looking for the smallest positive integer that is a multiple of 30 and can be formed using only the digits 0 and 2.\n",
      "\n",
      "## Step 2: Factorize 30\n",
      "The prime factorization of 30 is 2 * 3 * 5.\n",
      "\n",
      "## Step 3: Determine the requirements\n",
      "For a number to be a multiple of 30, it must be divisible by 2, 3, and 5.\n",
      "\n",
      "## Step 4: Consider the digits 0 and 2\n",
      "Since we can only use the digits 0 and 2, we can use them in any position in the number.\n",
      "\n",
      "## Step 5: Analyze the options\n",
      "To meet the requirements, we need to find the smallest multiple of 30 that can be formed using only 0 and 2.\n",
      "\n",
      "## Step 6: Start with the multiples of 30\n",
      "We can start by listing the multiples of 30 and checking which ones can be formed using only the digits 0 and 2.\n",
      "\n",
      "## Step 7: List multiples of 30\n",
      "Multiples of 30 are: 30, 60, 90, 120, 150, 180, 210, 240, 270, 300.\n",
      "\n",
      "## Step 8: Check if they meet the requirements\n",
      "Among the multiples of 30, we need to check which ones can be formed using only the digits 0 and 2.\n",
      "---\n",
      "**Step 9 Under Review:**\n",
      "## Step 9: Analyze the options\n",
      "120 meets the requirement as it can be formed using only the digits 0 and 2.\n",
      "---\n",
      "\n",
      "**Your Task:** Provide the reasoning and the single corrected step text below.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 453.03 toks/s, output: 85.69 toks/s]\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the least positive integer multiple of 30 that can be written with only the digits 0 and 2?\"\n",
    "\n",
    "steps = \"\"\"Understand the problem\\nWe are looking for the smallest positive integer that is a multiple of 30 and can be formed using only the digits 0 and 2.\\n\\n## Step 2: Factorize 30\\nThe prime factorization of 30 is 2 * 3 * 5.\\n\\n## Step 3: Determine the requirements\\nFor a number to be a multiple of 30, it must be divisible by 2, 3, and 5.\\n\\n## Step 4: Consider the digits 0 and 2\\nSince we can only use the digits 0 and 2, we can use them in any position in the number.\\n\\n## Step 5: Analyze the options\\nTo meet the requirements, we need to find the smallest multiple of 30 that can be formed using only 0 and 2.\\n\\n## Step 6: Start with the multiples of 30\\nWe can start by listing the multiples of 30 and checking which ones can be formed using only the digits 0 and 2.\\n\\n## Step 7: List multiples of 30\\nMultiples of 30 are: 30, 60, 90, 120, 150, 180, 210, 240, 270, 300.\\n\\n## Step 8: Check if they meet the requirements\\nAmong the multiples of 30, we need to check which ones can be formed using only the digits 0 and 2.\\n\\n## Step 9: Analyze the options\\n120 meets the requirement as it can be formed using only the digits 0 and 2.\\n\\n## Step 10: Verify if it is the smallest possible\\n120 is the smallest multiple of 30 that can be formed using only the digits 0 and 2.\\n\\nThe final answer is: $\\\\boxed{120}$\"\"\"\n",
    "steps = steps.split(\"\\n\\n\")\n",
    "particle = Particle()\n",
    "particle.trajectory = steps\n",
    "paritcles = [particle]\n",
    "output = self_refinement_simple(llm, paritcles, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reasoning: The error in Step 9 is that it states \"120 meets the requirement as it can be formed using only the digits 0 and 2\" without considering that the correct sequence of digits for 120 is 1, 2, 0 which includes the digit 1 that cannot be used according to the problem\\'s requirement.',\n",
       " 'Corrected Step 9:\\n## Step 9: Analyze the options\\n120 cannot meet the requirements as it cannot be formed using only the digits 0 and 2, because it includes the digit 1.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
